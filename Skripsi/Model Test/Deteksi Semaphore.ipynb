{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d9bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Inisialisasi MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
    "\n",
    "# Path ke folder dataset utama\n",
    "DATA_PATH = r\"G:\\aan\\Skripsi\\DatasetTest2\"\n",
    "\n",
    "# List label (nama folder)\n",
    "labels = os.listdir(DATA_PATH)\n",
    "# Pastikan urutannya benar jika diperlukan (A-Z)\n",
    "labels.sort() \n",
    "\n",
    "print(f\"Ditemukan label: {labels}\")\n",
    "\n",
    "# List untuk menampung data dan labelnya\n",
    "all_landmarks = []\n",
    "all_labels = []\n",
    "\n",
    "# =======================================================\n",
    "total_file_seharusnya = 0\n",
    "data_gagal_deteksi = [] # Menyimpan path file yang GAGAL terdeteksi oleh MediaPipe\n",
    "data_gagal_baca = []    # Menyimpan path file yang GAGAL dibaca/ditemukan (rusak/hilang)\n",
    "# =======================================================\n",
    "\n",
    "# Iterasi melalui setiap folder label (A, B, C, ...)\n",
    "for label_index, label in enumerate(labels):\n",
    "    folder_path = os.path.join(DATA_PATH, label)\n",
    "    \n",
    "    # Memastikan itu adalah direktori\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"Memproses folder: {label}\")\n",
    "\n",
    "    # Iterasi melalui setiap gambar dalam folder\n",
    "    for image_name in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, image_name)\n",
    "        \n",
    "        # =======================================================\n",
    "        #                   Hitungan total file\n",
    "        # =======================================================\n",
    "        total_file_seharusnya += 1\n",
    "        \n",
    "        try:\n",
    "            # Baca gambar\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            # =======================================================\n",
    "            #                   Deteksi Gagal Baca/Rusak\n",
    "            # =======================================================\n",
    "            if image is None:\n",
    "                print(f\"Gagal membaca/File rusak: {image_path}\")\n",
    "                data_gagal_baca.append(image_path)\n",
    "                continue\n",
    "            # =======================================================\n",
    "\n",
    "            # Konversi warna gambar dari BGR ke RGB\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Proses gambar untuk deteksi pose\n",
    "            results = pose.process(image_rgb)\n",
    "            \n",
    "            # Jika pose terdeteksi\n",
    "            if results.pose_landmarks:\n",
    "                # Ekstrak landmark dan ratakan (flatten) menjadi satu baris array\n",
    "                # 33 landmarks * 4 (x,y,z,vis) = 132 fitur\n",
    "                landmarks = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in results.pose_landmarks.landmark]).flatten()\n",
    "                \n",
    "                # Tambahkan data landmark dan labelnya\n",
    "                all_landmarks.append(landmarks)\n",
    "                all_labels.append(label_index) # Gunakan index (0 untuk A, 1 untuk B, dst.)\n",
    "            else:\n",
    "                # =======================================================\n",
    "                #                Deteksi Gagal Pose\n",
    "                # =======================================================\n",
    "                print(f\"Gagal deteksi pose di: {image_path}\")\n",
    "                data_gagal_deteksi.append(image_path)\n",
    "                # =======================================================\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saat memproses gambar {image_path}: {e}\")\n",
    "            data_gagal_baca.append(image_path) # Anggap error lain juga kegagalan baca/proses\n",
    "\n",
    "# Tutup objek Pose\n",
    "pose.close()\n",
    "\n",
    "# Konversi list menjadi numpy array untuk disimpan\n",
    "X = np.array(all_landmarks)\n",
    "y = np.array(all_labels)\n",
    "\n",
    "# Simpan data yang sudah diekstraksi\n",
    "np.save(\"data_landmarks.npy\", X)\n",
    "np.save(\"data_labels.npy\", y)\n",
    "\n",
    "# =======================================================\n",
    "#             Cetak Laporan Akhir\n",
    "# =======================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LAPORAN EKSTRAKSI DATA\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total file yang seharusnya diproses: {total_file_seharusnya}\")\n",
    "print(f\"Total data sukses diekstraksi: {X.shape[0]}\")\n",
    "print(f\"Total data yang HILANG/Gagal: {total_file_seharusnya - X.shape[0]}\")\n",
    "\n",
    "print(\"\\n--- Detail Kegagalan ---\")\n",
    "print(f\"Jumlah file GAGAL dibaca/rusak/error: {len(data_gagal_baca)}\")\n",
    "if data_gagal_baca:\n",
    "    print(\"Contoh file GAGAL baca/rusak:\")\n",
    "    for path in data_gagal_baca:\n",
    "        print(f\"  - {path}\")\n",
    "        \n",
    "print(f\"\\nJumlah file GAGAL deteksi oleh MediaPipe Pose: {len(data_gagal_deteksi)}\")\n",
    "if data_gagal_deteksi:\n",
    "    print(\"Contoh file GAGAL deteksi:\")\n",
    "    for path in data_gagal_deteksi:\n",
    "        print(f\"  - {path}\")\n",
    "\n",
    "# Verifikasi total kegagalan\n",
    "total_kegagalan_tercatat = len(data_gagal_baca) + len(data_gagal_deteksi)\n",
    "print(f\"\\nTotal Kegagalan Tercatat: {total_kegagalan_tercatat}\")\n",
    "print(f\"Perbedaan antara data hilang dan tercatat: { (total_file_seharusnya - X.shape[0]) - total_kegagalan_tercatat }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d866923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c3501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Muat Data Asli ---\n",
    "X = np.load('data_landmarks.npy')\n",
    "y = np.load('data_labels.npy')\n",
    "\n",
    "# --- Pembagian Data (SPLIT FIRST) ---\n",
    "# Memisahkan data asli menjadi Latih, Validasi, dan Uji.\n",
    "# Alokasi: 60% Latih, 20% Validasi, 20% Uji.\n",
    "\n",
    "# Pertama, pisahkan data uji (20%) dari data utama.\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Kedua, pisahkan sisa data (80%) menjadi data latih (60%) dan validasi (20%).\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.25, # 0.25 dari 80% adalah 20% dari total\n",
    "    random_state=42, \n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Ukuran Data Latih Awal: {X_train.shape[0]}\")\n",
    "print(f\"Ukuran Data Validasi: {X_val.shape[0]}\")\n",
    "print(f\"Ukuran Data Uji: {X_test.shape[0]}\")\n",
    "\n",
    "print(\"\\nMenyimpan data uji ke file...\")\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('y_test.npy', y_test)\n",
    "print(\"Data uji berhasil disimpan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f087c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Augmentasi HANYA pada Data Latih  ---\n",
    "def augment_landmarks(data, noise_level=0.005):\n",
    "    noise = np.random.normal(0, noise_level, data.shape)\n",
    "    return data + noise\n",
    "\n",
    "X_train_augmented = []\n",
    "y_train_augmented = []\n",
    "\n",
    "# Melakukan augmentasi hanya pada X_train\n",
    "for i in range(X_train.shape[0]):\n",
    "    original_sample = X_train[i]\n",
    "    original_label = y_train[i]\n",
    "    \n",
    "    # Tambahkan data asli ke list\n",
    "    X_train_augmented.append(original_sample)\n",
    "    y_train_augmented.append(original_label)\n",
    "    \n",
    "    # Membuat 10 variasi augmented untuk setiap data asli\n",
    "    for _ in range(10): \n",
    "        augmented_sample = augment_landmarks(original_sample)\n",
    "        X_train_augmented.append(augmented_sample)\n",
    "        y_train_augmented.append(original_label)\n",
    "\n",
    "# Konversi list augmentasi menjadi numpy array\n",
    "X_train_final = np.array(X_train_augmented)\n",
    "y_train_final = np.array(y_train_augmented)\n",
    "\n",
    "print(f\"Ukuran Data Latih Setelah Augmentasi: {X_train_final.shape[0]}\")\n",
    "\n",
    "\n",
    "# --- Pra-pemrosesan Akhir (One-Hot Encoding) ---\n",
    "num_classes = len(np.unique(y))\n",
    "y_train_cat = to_categorical(y_train_final, num_classes=num_classes)\n",
    "y_val_cat = to_categorical(y_val, num_classes=num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# --- Reshape data untuk input ke CNN 1D ---\n",
    "X_train_reshaped = np.expand_dims(X_train_final, axis=2)\n",
    "X_val_reshaped = np.expand_dims(X_val, axis=2)\n",
    "X_test_reshaped = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f380a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MEMBUAT DAN MELATIH MODEL LENET ---\n",
    "def create_lenet_1d(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=6, kernel_size=5, activation='relu', input_shape=input_shape),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Conv1D(filters=16, kernel_size=5, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Flatten(),\n",
    "        Dense(120, activation='relu'),\n",
    "        Dense(84, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Buat dan compile model\n",
    "lenet_model = create_lenet_1d(X_train_reshaped.shape[1:], num_classes)\n",
    "lenet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "lenet_model.summary()\n",
    "\n",
    "# Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "print(\"\\n--- Melatih Model LeNet-5 ---\")\n",
    "lenet_history = lenet_model.fit(\n",
    "    X_train_reshaped, y_train_cat,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_reshaped, y_val_cat),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# --- MENYIMPAN MODEL ---\n",
    "lenet_model.save(\"semaphore_lenet.h5\")\n",
    "print(\"\\nModel LeNet-5 berhasil disimpan.\")\n",
    "\n",
    "# --- EVALUASI AKHIR PADA DATA UJI ---\n",
    "print(\"\\n--- Evaluasi Akhir pada Data Uji (Tak Tersentuh) ---\")\n",
    "\n",
    "loss, accuracy = lenet_model.evaluate(X_test_reshaped, y_test_cat)\n",
    "print(f\"Akurasi pada Data Uji: {accuracy * 100:.2f}%\")\n",
    "print(f\"Loss pada Data Uji: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a294b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MEMBUAT DAN MELATIH MODEL VGG-LIKE ---\n",
    "def create_vgg_like_1d(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # Block 1\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', padding='same', input_shape=input_shape),\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Block 2\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Buat dan compile model\n",
    "vgg_model = create_vgg_like_1d(X_train_reshaped.shape[1:], num_classes)\n",
    "vgg_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "vgg_model.summary()\n",
    "\n",
    "# Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "print(\"\\n--- Melatih Model VGG-like ---\")\n",
    "vgg_history = vgg_model.fit(\n",
    "    X_train_reshaped, y_train_cat,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_reshaped, y_val_cat),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# --- MENYIMPAN MODEL ---\n",
    "vgg_model.save(\"semaphore_vgg_like.h5\")\n",
    "print(\"\\nModel VGG-like berhasil disimpan.\")\n",
    "\n",
    "# --- EVALUASI AKHIR PADA DATA UJI ---\n",
    "print(\"\\n--- Evaluasi Akhir pada Data Uji (Tak Tersentuh) ---\")\n",
    "loss, accuracy = vgg_model.evaluate(X_test_reshaped, y_test_cat)\n",
    "print(f\"Akurasi pada Data Uji: {accuracy * 100:.2f}%\")\n",
    "print(f\"Loss pada Data Uji: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d29808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Add\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# --- MEMBUAT DAN MELATIH MODEL RESNET-LIKE ---\n",
    "def residual_block(x, filters, kernel_size=3):\n",
    "    shortcut = x\n",
    "    y = Conv1D(filters, kernel_size, padding='same', activation='relu')(x)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv1D(filters, kernel_size, padding='same', activation='relu')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, kernel_size=1, padding='same')(shortcut)\n",
    "    y = Add()([shortcut, y])\n",
    "    return y\n",
    "\n",
    "def create_resnet_like_1d(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(32, 5, padding='same', activation='relu')(inputs)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = residual_block(x, filters=32)\n",
    "    x = residual_block(x, filters=32)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = residual_block(x, filters=64)\n",
    "    x = residual_block(x, filters=64)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Buat dan compile model\n",
    "resnet_model = create_resnet_like_1d(X_train_reshaped.shape[1:], num_classes)\n",
    "resnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_model.summary()\n",
    "\n",
    "# Callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "print(\"\\n--- Melatih Model ResNet-like ---\")\n",
    "resnet_history = resnet_model.fit(\n",
    "    X_train_reshaped, y_train_cat,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_reshaped, y_val_cat),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# --- MENYIMPAN MODEL ---\n",
    "resnet_model.save(\"semaphore_resnet_like.h5\")\n",
    "print(\"\\nModel ResNet-like berhasil disimpan.\")\n",
    "\n",
    "# --- EVALUASI AKHIR PADA DATA UJI ---\n",
    "print(\"\\n--- Evaluasi Akhir pada Data Uji (Tak Tersentuh) ---\")\n",
    "loss, accuracy = resnet_model.evaluate(X_test_reshaped, y_test_cat)\n",
    "print(f\"Akurasi pada Data Uji: {accuracy * 100:.2f}%\")\n",
    "print(f\"Loss pada Data Uji: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de377167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluasi Performa pada Data Uji\n",
    "print(\"\\n\\n--- Hasil Evaluasi pada Data Uji (Tak Tersentuh) ---\")\n",
    "loss_lenet, acc_lenet = lenet_model.evaluate(X_test_reshaped, y_test_cat, verbose=0)\n",
    "loss_vgg, acc_vgg = vgg_model.evaluate(X_test_reshaped, y_test_cat, verbose=0)\n",
    "loss_resnet, acc_resnet = resnet_model.evaluate(X_test_reshaped, y_test_cat, verbose=0)\n",
    "\n",
    "print(f\"LeNet-5      -> Akurasi: {acc_lenet*100:.2f}%, Loss: {loss_lenet:.4f}\")\n",
    "print(f\"VGG-like     -> Akurasi: {acc_vgg*100:.2f}%, Loss: {loss_vgg:.4f}\")\n",
    "print(f\"ResNet-like  -> Akurasi: {acc_resnet*100:.2f}%, Loss: {loss_resnet:.4f}\")\n",
    "\n",
    "# Perbandingan Konfigurasi Model (Jumlah Parameter)\n",
    "print(\"\\n--- Perbandingan Konfigurasi (Kompleksitas Model) ---\")\n",
    "print(f\"LeNet-5      -> Total Parameter: {lenet_model.count_params()}\")\n",
    "print(f\"VGG-like     -> Total Parameter: {vgg_model.count_params()}\")\n",
    "print(f\"ResNet-like  -> Total Parameter: {resnet_model.count_params()}\")\n",
    "\n",
    "# Visualisasi Kurva Pembelajaran\n",
    "print(\"\\n--- Menampilkan Grafik Perbandingan Kurva Pembelajaran ---\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Grafik Akurasi\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lenet_history.history['val_accuracy'], label='LeNet Validation Accuracy', linestyle='--')\n",
    "plt.plot(vgg_history.history['val_accuracy'], label='VGG Validation Accuracy', linestyle='--')\n",
    "plt.plot(resnet_history.history['val_accuracy'], label='ResNet Validation Accuracy', linestyle='--')\n",
    "plt.plot(lenet_history.history['accuracy'], label='LeNet Training Accuracy')\n",
    "plt.plot(vgg_history.history['accuracy'], label='VGG Training Accuracy')\n",
    "plt.plot(resnet_history.history['accuracy'], label='ResNet Training Accuracy')\n",
    "plt.title('Perbandingan Akurasi Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Akurasi')\n",
    "plt.legend()\n",
    "\n",
    "# Grafik Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lenet_history.history['val_loss'], label='LeNet Validation Loss', linestyle='--')\n",
    "plt.plot(vgg_history.history['val_loss'], label='VGG Validation Loss', linestyle='--')\n",
    "plt.plot(resnet_history.history['val_loss'], label='ResNet Validation Loss', linestyle='--')\n",
    "plt.plot(lenet_history.history['loss'], label='LeNet Training Loss')\n",
    "plt.plot(vgg_history.history['loss'], label='VGG Training Loss')\n",
    "plt.plot(resnet_history.history['loss'], label='ResNet Training Loss')\n",
    "plt.title('Perbandingan Loss Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e066d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Grafik Akurasi\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lenet_history.history['val_accuracy'], label='LeNet Validation Accuracy', linestyle='--')\n",
    "plt.plot(lenet_history.history['accuracy'], label='LeNet Training Accuracy')\n",
    "plt.title('Training and validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Akurasi')\n",
    "plt.legend()\n",
    "\n",
    "# Grafik Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(lenet_history.history['val_loss'], label='LeNet Validation Loss', linestyle='--')\n",
    "plt.plot(lenet_history.history['loss'], label='LeNet Training Loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a4e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Grafik Akurasi\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(vgg_history.history['val_accuracy'], label='VGG Validation Accuracy', linestyle='--')\n",
    "plt.plot(vgg_history.history['accuracy'], label='VGG Training Accuracy')\n",
    "plt.title('Perbandingan Akurasi Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Akurasi')\n",
    "plt.legend()\n",
    "\n",
    "# Grafik Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(vgg_history.history['val_loss'], label='VGG Validation Loss', linestyle='--')\n",
    "plt.plot(vgg_history.history['loss'], label='VGG Training Loss')\n",
    "plt.title('Perbandingan Loss Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4fc3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Grafik Akurasi\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(resnet_history.history['val_accuracy'], label='ResNet Validation Accuracy', linestyle='--')\n",
    "plt.plot(resnet_history.history['accuracy'], label='ResNet Training Accuracy')\n",
    "plt.title('Perbandingan Akurasi Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Akurasi')\n",
    "plt.legend()\n",
    "\n",
    "# Grafik Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(resnet_history.history['val_loss'], label='ResNet Validation Loss', linestyle='--')\n",
    "plt.plot(resnet_history.history['loss'], label='ResNet Training Loss')\n",
    "plt.title('Perbandingan Loss Model')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# --- MEMUAT MODEL DAN DATA UJI ---\n",
    "print(\"Memuat model dan data uji...\")\n",
    "\n",
    "# Muat model terbaik\n",
    "model = load_model('semaphore_lenet.h5')\n",
    "\n",
    "try:\n",
    "    X_test = np.load('X_test.npy')\n",
    "    y_test = np.load('y_test.npy')\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nPERINGATAN: File X_test.npy atau y_test.npy tidak ditemukan.\")\n",
    "    print(\"Anda perlu menyimpan data uji dari skrip training terlebih dahulu.\")\n",
    "    # Jika file tidak ada, eksekusi dihentikan\n",
    "    exit()\n",
    "\n",
    "# Reshape data uji untuk prediksi\n",
    "X_test_reshaped = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "print(\"Data berhasil dimuat.\")\n",
    "\n",
    "# --- MELAKUKAN PREDIKSI PADA DATA UJI ---\n",
    "print(\"\\nMelakukan prediksi...\")\n",
    "predictions = model.predict(X_test_reshaped)\n",
    "# Mengambil indeks kelas dengan probabilitas tertinggi untuk setiap prediksi\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "true_classes = y_test\n",
    "\n",
    "# Membuat daftar label huruf dari A-Z\n",
    "labels = [chr(i) for i in range(ord('A'), ord('Z') + 1)]\n",
    "\n",
    "# --- MENAMPILKAN LAPORAN KLASIFIKASI & CONFUSION MATRIX ---\n",
    "\n",
    "# Laporan Klasifikasi (Precision, Recall, F1-Score)\n",
    "print(\"\\nLaporan Klasifikasi Lengkap:\")\n",
    "print(classification_report(true_classes, predicted_classes, target_names=labels))\n",
    "\n",
    "# Confusion Matrix untuk visualisasi kesalahan\n",
    "print(\"\\nMembuat Confusion Matrix...\")\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix - Kesalahan Prediksi Model')\n",
    "plt.ylabel('Kelas Sebenarnya (True Label)')\n",
    "plt.xlabel('Kelas Prediksi (Predicted Label)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- MENCARI DAN MENAMPILKAN KESALAHAN SPESIFIK ---\n",
    "print(\"\\n--- Analisis Kesalahan Spesifik ---\")\n",
    "error_count = 0\n",
    "for i in range(len(true_classes)):\n",
    "    if predicted_classes[i] != true_classes[i]:\n",
    "        error_count += 1\n",
    "        true_label = labels[true_classes[i]]\n",
    "        predicted_label = labels[predicted_classes[i]]\n",
    "        confidence = predictions[i][predicted_classes[i]] * 100\n",
    "        \n",
    "        print(f\"{error_count}. Model salah memprediksi:\")\n",
    "        print(f\"   > Huruf Sebenarnya: '{true_label}'\")\n",
    "        print(f\"   > Diprediksi sebagai: '{predicted_label}' (Keyakinan: {confidence:.2f}%)\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "if error_count == 0:\n",
    "    print(\"Selamat! Tidak ditemukan kesalahan pada data uji.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# --- MEMUAT MODEL DAN DATA UJI ---\n",
    "print(\"Memuat model dan data uji...\")\n",
    "\n",
    "# Muat model terbaik\n",
    "model = load_model('semaphore_vgg_like.h5')\n",
    "\n",
    "try:\n",
    "    X_test = np.load('X_test.npy')\n",
    "    y_test = np.load('y_test.npy')\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nPERINGATAN: File X_test.npy atau y_test.npy tidak ditemukan.\")\n",
    "    print(\"Anda perlu menyimpan data uji dari skrip training terlebih dahulu.\")\n",
    "    # Jika file tidak ada, eksekusi dihentikan\n",
    "    exit()\n",
    "\n",
    "# Reshape data uji untuk prediksi\n",
    "X_test_reshaped = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "print(\"Data berhasil dimuat.\")\n",
    "\n",
    "# --- MELAKUKAN PREDIKSI PADA DATA UJI ---\n",
    "print(\"\\nMelakukan prediksi...\")\n",
    "predictions = model.predict(X_test_reshaped)\n",
    "# Mengambil indeks kelas dengan probabilitas tertinggi untuk setiap prediksi\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "true_classes = y_test\n",
    "\n",
    "# Membuat daftar label huruf dari A-Z\n",
    "labels = [chr(i) for i in range(ord('A'), ord('Z') + 1)]\n",
    "\n",
    "# --- MENAMPILKAN LAPORAN KLASIFIKASI & CONFUSION MATRIX ---\n",
    "\n",
    "# Laporan Klasifikasi (Precision, Recall, F1-Score)\n",
    "print(\"\\nLaporan Klasifikasi Lengkap:\")\n",
    "print(classification_report(true_classes, predicted_classes, target_names=labels))\n",
    "\n",
    "# Confusion Matrix untuk visualisasi kesalahan\n",
    "print(\"\\nMembuat Confusion Matrix...\")\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix - Kesalahan Prediksi Model')\n",
    "plt.ylabel('Kelas Sebenarnya (True Label)')\n",
    "plt.xlabel('Kelas Prediksi (Predicted Label)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- MENCARI DAN MENAMPILKAN KESALAHAN SPESIFIK ---\n",
    "print(\"\\n--- Analisis Kesalahan Spesifik ---\")\n",
    "error_count = 0\n",
    "for i in range(len(true_classes)):\n",
    "    if predicted_classes[i] != true_classes[i]:\n",
    "        error_count += 1\n",
    "        true_label = labels[true_classes[i]]\n",
    "        predicted_label = labels[predicted_classes[i]]\n",
    "        confidence = predictions[i][predicted_classes[i]] * 100\n",
    "        \n",
    "        print(f\"{error_count}. Model salah memprediksi:\")\n",
    "        print(f\"   > Huruf Sebenarnya: '{true_label}'\")\n",
    "        print(f\"   > Diprediksi sebagai: '{predicted_label}' (Keyakinan: {confidence:.2f}%)\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "if error_count == 0:\n",
    "    print(\"Selamat! Tidak ditemukan kesalahan pada data uji.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# --- MEMUAT MODEL DAN DATA UJI ---\n",
    "print(\"Memuat model dan data uji...\")\n",
    "\n",
    "# Muat model terbaik\n",
    "model = load_model('semaphore_resnet_like.h5')\n",
    "\n",
    "try:\n",
    "    X_test = np.load('X_test.npy')\n",
    "    y_test = np.load('y_test.npy')\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nPERINGATAN: File X_test.npy atau y_test.npy tidak ditemukan.\")\n",
    "    print(\"Anda perlu menyimpan data uji dari skrip training terlebih dahulu.\")\n",
    "    # Jika file tidak ada, eksekusi dihentikan\n",
    "    exit()\n",
    "\n",
    "# Reshape data uji untuk prediksi\n",
    "X_test_reshaped = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "print(\"Data berhasil dimuat.\")\n",
    "\n",
    "# --- MELAKUKAN PREDIKSI PADA DATA UJI ---\n",
    "print(\"\\nMelakukan prediksi...\")\n",
    "predictions = model.predict(X_test_reshaped)\n",
    "# Mengambil indeks kelas dengan probabilitas tertinggi untuk setiap prediksi\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "true_classes = y_test\n",
    "\n",
    "# Membuat daftar label huruf dari A-Z\n",
    "labels = [chr(i) for i in range(ord('A'), ord('Z') + 1)]\n",
    "\n",
    "# --- MENAMPILKAN LAPORAN KLASIFIKASI & CONFUSION MATRIX ---\n",
    "\n",
    "# Laporan Klasifikasi (Precision, Recall, F1-Score)\n",
    "print(\"\\nLaporan Klasifikasi Lengkap:\")\n",
    "print(classification_report(true_classes, predicted_classes, target_names=labels))\n",
    "\n",
    "# Confusion Matrix untuk visualisasi kesalahan\n",
    "print(\"\\nMembuat Confusion Matrix...\")\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=labels, yticklabels=labels)\n",
    "plt.title('Confusion Matrix - Kesalahan Prediksi Model')\n",
    "plt.ylabel('Kelas Sebenarnya (True Label)')\n",
    "plt.xlabel('Kelas Prediksi (Predicted Label)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- MENCARI DAN MENAMPILKAN KESALAHAN SPESIFIK ---\n",
    "print(\"\\n--- Analisis Kesalahan Spesifik ---\")\n",
    "error_count = 0\n",
    "for i in range(len(true_classes)):\n",
    "    if predicted_classes[i] != true_classes[i]:\n",
    "        error_count += 1\n",
    "        true_label = labels[true_classes[i]]\n",
    "        predicted_label = labels[predicted_classes[i]]\n",
    "        confidence = predictions[i][predicted_classes[i]] * 100\n",
    "        \n",
    "        print(f\"{error_count}. Model salah memprediksi:\")\n",
    "        print(f\"   > Huruf Sebenarnya: '{true_label}'\")\n",
    "        print(f\"   > Diprediksi sebagai: '{predicted_label}' (Keyakinan: {confidence:.2f}%)\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "if error_count == 0:\n",
    "    print(\"Selamat! Tidak ditemukan kesalahan pada data uji.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f51a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Muat model yang sudah dilatih\n",
    "model = load_model('semaphore_lenet.h5')\n",
    "\n",
    "# Label huruf\n",
    "LABELS = [chr(i) for i in range(ord('A'), ord('Z') + 1)]\n",
    "\n",
    "# Inisialisasi MediaPipe\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Konversi warna dan proses dengan MediaPipe\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = pose.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Ekstraksi landmark jika terdeteksi\n",
    "    if results.pose_landmarks:\n",
    "        # Gambar landmark pada frame\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Siapkan data untuk prediksi\n",
    "            landmarks = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in results.pose_landmarks.landmark]).flatten()\n",
    "            landmarks = np.expand_dims(landmarks, axis=0) # -> (1, 132)\n",
    "            landmarks = np.expand_dims(landmarks, axis=2) # -> (1, 132, 1)\n",
    "            \n",
    "            # Lakukan prediksi\n",
    "            prediction = model.predict(landmarks)\n",
    "            predicted_class = np.argmax(prediction)\n",
    "            confidence = np.max(prediction)\n",
    "            \n",
    "            # Tampilkan hasil\n",
    "            text = f\"Huruf: {LABELS[predicted_class]} ({confidence*100:.2f}%)\"\n",
    "            cv2.putText(image, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saat prediksi: {e}\")\n",
    "\n",
    "    cv2.imshow('Deteksi Semaphore Pramuka', image)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
